import { GoogleGenAI, Modality } from '@google/genai';

export class GeminiLiveOfficial {
  constructor(options) {
    this.options = options;
    this.geminiSession = null;
    this.responseQueue = [];
    this.onReady = options.onReady;
    this.onServerContent = options.onServerContent;
    this.onError = options.onError;
    this.onClose = options.onClose;
  }

  async connect() {
    try {
      console.log('üîÑ Connecting to Gemini Live API using official @google/genai...');
      console.log('üîß Model:', this.options.model || 'gemini-2.5-flash-preview-native-audio-dialog');
      console.log('üîë API Key length:', this.options.apiKey ? this.options.apiKey.length : 'MISSING');
      
      // Initialize GoogleGenAI client
      const ai = new GoogleGenAI({
        apiKey: this.options.apiKey
      });
      
      // Prepare configuration according to official API  
      const config = {
        responseModalities: [Modality.AUDIO],
        speechConfig: this.options.speechConfig || {
          voiceConfig: { 
            prebuiltVoiceConfig: { 
              voiceName: "Puck" 
            } 
          }
        },
        systemInstruction: this.options.systemInstruction || {
          parts: [{ text: "You are a helpful AI assistant on a phone call. Wait for the caller to speak first, then respond naturally and conversationally." }]
        },
        // Enable proper Voice Activity Detection to handle natural conversation flow
        // This allows the caller to speak first and Gemini will respond naturally
        realtimeInputConfig: {
          automaticActivityDetection: {
            start_of_speech_sensitivity: 'START_SENSITIVITY_MEDIUM',
            end_of_speech_sensitivity: 'END_SENSITIVITY_MEDIUM', 
            silence_duration_ms: 1500,  // Wait 1.5 seconds of silence before considering speech ended
            prefix_padding_ms: 300      // Include 300ms before detected speech
          }
        },
        inputAudioTranscription: {},
        outputAudioTranscription: {}
      };

      console.log('üîß Config:', JSON.stringify(config, null, 2));

      // Connect to Live API with callbacks
      this.geminiSession = await ai.live.connect({
        model: this.options.model || 'gemini-2.5-flash-preview-native-audio-dialog',
        callbacks: {
          onopen: () => {
            console.log('‚úÖ Gemini Live session established');
            if (this.onReady) this.onReady();
          },
          onmessage: (message) => {
            console.log('üì® Received message from Gemini:', JSON.stringify(message, null, 2).substring(0, 200) + '...');
            this.responseQueue.push(message);
            if (this.onServerContent) this.onServerContent(message);
          },
          onerror: (error) => {
            console.error('‚ùå Gemini Live session error:', error);
            if (this.onError) this.onError(error);
          },
          onclose: (event) => {
            console.log('‚ÑπÔ∏è Gemini Live session closed:', event.reason);
            if (this.onClose) this.onClose();
          }
        },
        config: config
      });
      
      console.log('üéâ Gemini Live session connected successfully using official API');
      return true;
    } catch (error) {
      console.error('‚ùå Failed to connect to Gemini Live:', error);
      if (this.onError) this.onError(error);
      return false;
    }
  }

  // Method for sending text (only when responding to user input)
  sendText(text) {
    if (!this.geminiSession) {
      console.error('Cannot send text, Gemini session not ready.');
      return false;
    }
    
    try {
      console.log(`ÔøΩÔ∏è Sending text to Gemini: "${text}"`);
      this.geminiSession.sendClientContent({
        turns: [{ role: 'user', parts: [{ text }] }],
        turnComplete: true
      });
      return true;
    } catch (error) {
      console.error('‚ùå Error sending text to Gemini:', error);
      return false;
    }
  }

  // Method for streaming audio
  sendAudio(base64AudioChunk) {
    if (!this.geminiSession) {
      console.error('Cannot send audio, Gemini session not ready.');
      return false;
    }
    
    try {
      this.geminiSession.sendRealtimeInput({
        audio: { 
          data: base64AudioChunk,
          mimeType: "audio/pcm;rate=16000" 
        },
      });
      return true;
    } catch (error) {
      console.error('‚ùå Error sending audio to Gemini:', error);
      return false;
    }
  }

  // Method for handling function call responses
  sendFunctionResponse(functionName, response) {
    if (!this.geminiSession) {
      console.error('Cannot send function response, Gemini session not ready.');
      return false;
    }
    
    try {
      console.log(`üîß Sending function response for "${functionName}" to Gemini`);
      this.geminiSession.sendClientContent({
        turns: [{
          role: 'function',
          parts: [{ functionResponse: { name: functionName, response } }]
        }],
        turnComplete: true
      });
      return true;
    } catch (error) {
      console.error('‚ùå Error sending function response to Gemini:', error);
      return false;
    }
  }

  // Method to end the audio stream (useful for VAD)
  sendAudioStreamEnd() {
    if (!this.geminiSession) return false;
    
    try {
      this.geminiSession.sendRealtimeInput({ audioStreamEnd: true });
      console.log('üîö Sent audio stream end signal');
      return true;
    } catch (error) {
      console.error('‚ùå Error sending audio stream end:', error);
      return false;
    }
  }

  // Close the session
  close() {
    if (this.geminiSession) {
      try {
        this.geminiSession.close();
        console.log('üëã Gemini session closed');
      } catch (error) {
        console.error('‚ùå Error closing Gemini session:', error);
      } finally {
        this.geminiSession = null;
      }
    }
  }
}